{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流式输出的细节\n",
    "\n",
    "词表的token对应的乱码，怎么正确打印的中英文，对后续自己逐字检验效果的时候有00借鉴意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常规方式加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, Qwen2ForCausalLM\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "model_path = 'D:\\learning\\python\\pretrain_checkpoint\\Qwen2.5-1.5B-Instruct'\n",
    "model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "text = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个人工智能助手\"},\n",
    "    {\"role\": \"user\", \"content\": '写一个谜语'}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     max_new_tokens=64,\n",
    "#     do_sample=True,\n",
    "#     **model_inputs,\n",
    "# )\n",
    "# generated_ids1 = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids1, skip_special_tokens=True)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextStreamer 基础流式输出"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    streamer=streamer,\n",
    "    **model_inputs,\n",
    ")\n",
    "generated_ids1 = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids1, skip_special_tokens=True)[0]\n",
    "print(\"最终结果：\", response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from typing import TYPE_CHECKING, Optional\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from ..models.auto import AutoTokenizer\n",
    "\n",
    "class BaseStreamer:\n",
    "    def put(self, value):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def end(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStreamer(BaseStreamer):\n",
    "    \"\"\"\n",
    "    Simple text streamer that prints the token(s) to stdout as soon as entire words are formed.\n",
    "\n",
    "    <Tip warning={true}>\n",
    "\n",
    "    The API for the streamer classes is still under development and may change in the future.\n",
    "\n",
    "    </Tip>\n",
    "\n",
    "    Parameters:\n",
    "        tokenizer (`AutoTokenizer`):\n",
    "            The tokenized used to decode the tokens.\n",
    "        skip_prompt (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.\n",
    "        decode_kwargs (`dict`, *optional*):\n",
    "            Additional keyword arguments to pass to the tokenizer's `decode` method.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "        >>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "        >>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "        >>> streamer = TextStreamer(tok)\n",
    "\n",
    "        >>> # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "        >>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\n",
    "        An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, **decode_kwargs):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.skip_prompt = skip_prompt  # 是否打印prompt\n",
    "        self.decode_kwargs = decode_kwargs  # 解码参数\n",
    "\n",
    "        # 用于记录流式输出过程中的变量\n",
    "        self.token_cache = []   # 缓存token\n",
    "        self.print_len = 0       # 记录上次打印位置\n",
    "        self.next_tokens_are_prompt = True  # 第一次为True，后续为False，记录当前调用put()时是否为prompt\n",
    "\n",
    "    def put(self, value):\n",
    "        \"\"\"\n",
    "        传入token后解码，然后在他们形成一个完整的词时将其打印到标准输出stdout\n",
    "        \"\"\"\n",
    "        # 这个类只支持 batch_size=1\n",
    "        # 第一次运行.put()时，value=input_id，此时检测batch大小，input_id.shape：(batch_size, seq_len)\n",
    "        if len(value.shape) > 1 and value.shape[0] > 1:\n",
    "            raise ValueError(\"TextStreamer only supports batch size 1\")\n",
    "        # 如果输入batch形式，但是batch_size=1，取第一个batch序列\n",
    "        elif len(value.shape) > 1:\n",
    "            value = value[0]\n",
    "\n",
    "        # 第一次输入的视为prompt，用参数判断是否打印prompt\n",
    "        if self.skip_prompt and self.next_tokens_are_prompt:\n",
    "            self.next_tokens_are_prompt = False\n",
    "            return\n",
    "\n",
    "        # 将新token添加到缓存，并解码整个token\n",
    "        self.token_cache.extend(value.tolist())\n",
    "        text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n",
    "\n",
    "        # 如果token以换行符结尾，则清空缓存\n",
    "        if text.endswith(\"\\n\"):\n",
    "            printable_text = text[self.print_len :]\n",
    "            self.token_cache = []\n",
    "            self.print_len = 0\n",
    "        # 如果最后一个token是中日韩越统一表意文字，则打印该字符\n",
    "        elif len(text) > 0 and self._is_chinese_char(ord(text[-1])):\n",
    "            printable_text = text[self.print_len :]\n",
    "            self.print_len += len(printable_text)\n",
    "        # 否则，打印直到最后一个空格字符（简单启发式，防止输出token是不完整的单词，在前一个词解码完毕后在打印）\n",
    "        # text=\"Hello!\"，此时不打印。text=\"Hello! I\"，打印Hello!\n",
    "        else:\n",
    "            printable_text = text[self.print_len : text.rfind(\" \") + 1]\n",
    "            self.print_len += len(printable_text)\n",
    "\n",
    "        self.on_finalized_text(printable_text)\n",
    "\n",
    "    def end(self):\n",
    "        \"\"\"清空缓存，并打印换行符到标准输出stdout\"\"\"\n",
    "        # 如果缓存不为空，则解码缓存，并打印直到最后一个空格字符\n",
    "        if len(self.token_cache) > 0:\n",
    "            text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n",
    "            printable_text = text[self.print_len :]\n",
    "            self.token_cache = []\n",
    "            self.print_len = 0\n",
    "        else:\n",
    "            printable_text = \"\"\n",
    "\n",
    "        self.next_tokens_are_prompt = True\n",
    "        self.on_finalized_text(printable_text, stream_end=True)\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        # flush=True，立即刷新缓冲区，实时显示，取消缓冲存在的延迟\n",
    "        # 如果stream_end为True，则打印换行符\n",
    "        print(text, flush=True, end=\"\" if not stream_end else None)\n",
    "\n",
    "    def _is_chinese_char(self, cp):\n",
    "        \"\"\"检查CP是否是CJK字符\"\"\"\n",
    "        # 这个定义了一个\"chinese character\"为CJK Unicode块中的任何内容：\n",
    "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
    "\n",
    "        # 我们使用Unicode块定义，因为这些字符是唯一的，并且它们是所有主要语言的常见字符。\n",
    "        # 注意，CJK Unicode块不仅仅是日语和韩语字符，\n",
    "        # 尽管它的名字如此，现代韩语的Hangul字母是另一个块，\n",
    "        # 日语的Hiragana和Katakana也是另一个块，\n",
    "        # 那些字母用于写space-separated words，所以它们不被特别处理，像其他语言一样处理\n",
    "        if (\n",
    "            (cp >= 0x4E00 and cp <= 0x9FFF)\n",
    "            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
    "            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
    "            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
    "            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
    "            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
    "            or (cp >= 0xF900 and cp <= 0xFAFF)\n",
    "            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
    "        ):  \n",
    "            return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextIterateStreamer 迭代打印流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "generation_kwargs = dict(model_inputs, streamer=streamer, max_new_tokens=100)\n",
    "# 在单独的线程中调用.generate()\n",
    "thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from typing import TYPE_CHECKING, Optional\n",
    "\n",
    "class TextIteratorStreamer(TextStreamer):\n",
    "    \"\"\"\n",
    "    将打印就绪的文本存储在队列中的流式处理器,可以被下游应用程序作为迭代器使用。这对于需要以非阻塞方式访问生成文本的应用程序很有用\n",
    "    (例如在交互式 Gradio 演示中)。\n",
    "\n",
    "    Parameters:\n",
    "        tokenizer (`AutoTokenizer`):\n",
    "            The tokenized used to decode the tokens.\n",
    "        skip_prompt (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.\n",
    "        timeout (`float`, *optional*):\n",
    "            文本队列的超时时间。如果为`None`,队列将无限期阻塞。当在单独的线程中调用`.generate()`时,这对于处理异常很有用。\n",
    "        decode_kwargs (`dict`, *optional*):\n",
    "            Additional keyword arguments to pass to the tokenizer's `decode` method.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "        >>> from threading import Thread\n",
    "\n",
    "        >>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "        >>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "        >>> streamer = TextIteratorStreamer(tok)\n",
    "\n",
    "        >>> # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
    "        >>> generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n",
    "        >>> thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "        >>> thread.start()\n",
    "        >>> generated_text = \"\"\n",
    "        >>> for new_text in streamer:\n",
    "        ...     generated_text += new_text\n",
    "        >>> generated_text\n",
    "        'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n",
    "    ):\n",
    "        super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n",
    "        self.text_queue = Queue()  # 文本队列\n",
    "        self.stop_signal = None  # 停止信号\n",
    "        self.timeout = timeout  # 队列超时时间\n",
    "\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        \"\"\"Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\"\"\"\n",
    "        # 将新文本放入队列\n",
    "        self.text_queue.put(text, timeout=self.timeout)\n",
    "        # 如果流结束，则将停止信号放入队列\n",
    "        if stream_end:\n",
    "            self.text_queue.put(self.stop_signal, timeout=self.timeout)\n",
    "\n",
    "    # 调用自己，返回迭代器\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # 调用一次迭代器，就从队列中获取一段文本，如果超时则抛出异常，默认self.timeout，表示不限时长\n",
    "        value = self.text_queue.get(timeout=self.timeout)\n",
    "        # 如果获取到停止信号,则抛出StopIteration异常表示迭代结束\n",
    "        if value == self.stop_signal:\n",
    "            raise StopIteration()\n",
    "        # 否则返回获取到的文本\n",
    "        else:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本地代码模型加载并前端展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streamlit 输出显示\n",
    "\n",
    "见同级目录 streamlit_app_base.py 文件，使用 streamlit run streamlit_app_base.py 命令启动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradio测试流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from threading import Thread\n",
    "from typing import List\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, Qwen2ForCausalLM\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "model_path = 'D:\\learning\\python\\pretrain_checkpoint\\Qwen2.5-1.5B-Instruct'\n",
    "model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def chat(question, history):\n",
    "   message = [{\"role\": \"system\", \"content\": \"你是一个人工智能助手\"}]\n",
    "   if not history:\n",
    "       message.append({\"role\": \"user\", \"content\": question})\n",
    "   else:\n",
    "       for i in history:\n",
    "            message.append({\"role\": \"user\", \"content\": i[0]})\n",
    "            message.append({\"role\": \"assistant\", \"content\": i[1]})\n",
    "       message.append({\"role\": \"user\", \"content\": question})\n",
    "   text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "   encoding = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "   streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "   generation_kwargs = dict(encoding, streamer=streamer, max_new_tokens=1024)\n",
    "   thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "   thread.start()\n",
    "\n",
    "   response = \"\"\n",
    "   for text in streamer:\n",
    "       response += text\n",
    "       yield response\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "   fn=chat,\n",
    "   title=\"聊天机器人\",\n",
    "   description=\"输入问题开始对话\"\n",
    ")\n",
    "\n",
    "demo.queue().launch(\n",
    "    server_name=\"0.0.0.0\",  # 如果不好使，可以尝试换成localhost或自身真正的ip地址\n",
    "    share=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vllm 部署模型并前端展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streamlit 输出显示\n",
    "\n",
    "见同级目录 streamlit_app_vllm.py 文件，使用 streamlit run streamlit_app_vllm.py 命令启动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gradio测试流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "\n",
    "def chat(question, history):\n",
    "    message = [{\"role\": \"system\", \"content\": \"你是一个人工智能助手\"}]\n",
    "    if not history:\n",
    "        message.append({\"role\": \"user\", \"content\": question})\n",
    "    else:\n",
    "        for i in history:\n",
    "                message.append({\"role\": \"user\", \"content\": i[0]})\n",
    "                message.append({\"role\": \"assistant\", \"content\": i[1]})\n",
    "        message.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    openai_api_key = \"EMPTY\"\n",
    "    openai_api_base = \"http://0.0.0.0:5001/v1\"  # 换成自己的ip+端口\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        base_url=openai_api_base,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen2.5\",\n",
    "        messages=message,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is None:\n",
    "            response_text += \"\"\n",
    "            yield response_text\n",
    "        else:\n",
    "            response_text += chunk.choices[0].delta.content\n",
    "            yield response_text\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "   fn=chat,\n",
    "   title=\"聊天机器人\",\n",
    "   description=\"输入问题开始对话\"\n",
    ")\n",
    "\n",
    "demo.queue().launch(\n",
    "    server_name=\"0.0.0.0\",  # 如果不好使，可以尝试换成localhost或自身真正的ip地址\n",
    "    share=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
