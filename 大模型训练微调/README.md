# 大模型训练微调朴素代码

待制作

# 训练显存优化实验报告

记录总结大模型微调时的显存占用情况，便于读者进行实验时参考。

## 实验配置
- 基础模型: Qwen2.5-7B-Instruct
- 硬件环境: 8*A100 GPU
- 对比维度:
  - 微调方式: LoRA(rank=16) vs 全参微调
  - 并行策略: DDP vs DeepSpeed
  - 注意力计算模式: eager/sdpa/flash_attention
  - 序列长度: 2048/4096

## 显存占用数据
### LoRA微调（单位：GB/卡）
| 训练配置               | 4096序列 | 2048序列 |
|------------------------|---------|---------|
| DeepSpeed + eager      | OOM     | 52.31   |
| DeepSpeed + sdpa       | 57.69   | 33.93   |
| DeepSpeed + flash      | 54.57   | 32.66   |
| DDP + eager           | OOM     | 54.73   |
| DDP + sdpa            | 57.12   | 36.73   |
| DDP + flash           | 55.71   | 35.99   |

### 全参微调（单位：GB/卡）
| 训练配置               | 4096序列 | 2048序列 |
|------------------------|---------|---------|
| DeepSpeed + eager      | OOM     | 71.25   |
| DeepSpeed + sdpa       | 65.87   | 48.28   |
| DeepSpeed + flash      | 62.09   | 46.28   |
| DDP                   | OOM     | -       |

### 实验结论
1. **序列长度影响**：
    - 序列长度从2048提升到4096时，显存占用增长约60-70%
    - 使用flash_attention可有效缓解长序列显存压力，序列越长，节省显存越多

2. **注意力优化对比**：
    - 计算效率：flash_attention > sdpa > eager
    - 显存节省：使用 flash_attention 相对 sdpa 只能轻微缓解显存压力，eager 完全无法使用（qwen2.5默认使用sdpa）

3. **微调方式差异**：
    - 全参微调相比LoRA多消耗约7GB/卡显存
    - 主要差异来自优化器状态：
        ```
        全参优化器占用 ≈ 7GB*2(bf16)*4(动量+状态) = 56GB（分8卡后≈7GB/卡）
        LoRA优化器参数可忽略不计
        ```

4. **并行策略选择**：
    - deepspeed 可以将 `梯度+优化器+模型参数` 并行，全参微调时优化器占大头，所以deepspeed缓解很多
    - lora+小模型训练时，模型参数+优化器都不多，数据占大头，所以deepspeed和ddp差距不大