{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, Qwen2ForCausalLM\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "model_path = 'D:\\learning\\python\\pretrain_checkpoint\\Qwen2.5-1.5B-Instruct'\n",
    "model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个人工智能助手\"},\n",
    "    {\"role\": \"user\", \"content\": '5个字夸一下微博'}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=True)\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMPLE 随机采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息交流快<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 调用函数生成\n",
    "generated_ids = model.generate(\n",
    "    top_k=5,\n",
    "    top_p=1,\n",
    "    max_new_tokens=64,\n",
    "    num_return_sequences=1,\n",
    "    output_scores=True,\n",
    "    output_logits=True,\n",
    "    return_dict_in_generate=True,\n",
    "    do_sample=True,\n",
    "    **model_inputs,\n",
    ")\n",
    "print(tokenizer.decode(generated_ids['sequences'][0][model_inputs.input_ids.shape[1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息交流快<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "27369 信息 0.35\n",
      "43815 内容 0.14\n",
      "50007 更新 0.06\n",
      "93149 分享 0.1\n",
      "100848 精彩 0.35\n",
      "----------------------------------------------------------------------------------------------------\n",
      "26288 大 0.06\n",
      "50007 更新 0.06\n",
      "93149 分享 0.08\n",
      "100667 及时 0.11\n",
      "101069 交流 0.22\n",
      "104793 传递 0.46\n",
      "----------------------------------------------------------------------------------------------------\n",
      "80942 广 0.02\n",
      "99234 快 0.69\n",
      "100133 平台 0.16\n",
      "105066 无限 0.03\n",
      "105499 便捷 0.1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1773 。 0.12\n",
      "3837 ， 0.0\n",
      "6313 ！ 0.0\n",
      "29524 如 0.01\n",
      "151645 <|im_end|> 0.87\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 随机采样每步输出\n",
    "print(tokenizer.decode(generated_ids['sequences'][0][model_inputs.input_ids.shape[1]:]))\n",
    "print('-'*100)\n",
    "for idx in range(len(generated_ids['scores'])):\n",
    "    all_token_ids = (generated_ids['scores'][idx][0] != float('-inf')).nonzero().view(-1)\n",
    "    score = generated_ids['scores'][idx][0][all_token_ids]\n",
    "    for i, token_id in enumerate(all_token_ids):\n",
    "        score = torch.softmax(generated_ids['scores'][idx][0][all_token_ids], dim=-1)\n",
    "        print(token_id.item(), tokenizer.decode(token_id), round(score[i].item(), 2))\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor,\n",
    "    logits_processor: LogitsProcessorList,\n",
    "    stopping_criteria: StoppingCriteriaList,\n",
    "    generation_config: GenerationConfig,\n",
    "    synced_gpus: bool,\n",
    "    streamer: Optional[\"BaseStreamer\"],\n",
    "    **model_kwargs,\n",
    ") -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n",
    "    r\"\"\"\n",
    "    使用 **multinomial sampling** 生成序列的 token ids, 适用于具有语言建模头的文本解码器、文本到文本、语音到文本和视觉到文本模型。\n",
    "\n",
    "    Parameters:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            用于生成序列的输入序列。\n",
    "        logits_processor (`LogitsProcessorList`):\n",
    "            一个 [`LogitsProcessorList`] 实例。包含从 [`LogitsProcessor`] 派生的类的实例列表，用于在每个生成步骤中修改语言建模头的预测分数。\n",
    "        stopping_criteria (`StoppingCriteriaList`):\n",
    "            一个 [`StoppingCriteriaList`] 实例。包含从 [`StoppingCriteria`] 派生的类的实例列表，用于告诉生成循环是否应该停止。\n",
    "        generation_config ([`~generation.GenerationConfig`]):\n",
    "            用于解码方法的生成配置。\n",
    "        synced_gpus (`bool`):\n",
    "            是否继续运行 while 循环直到 max_length (需要避免与 `FullyShardedDataParallel` 和 DeepSpeed ZeRO Stage 3 的死锁)。\n",
    "        streamer (`BaseStreamer`, *optional*):\n",
    "            流式对象，用于流式生成序列。生成的 token 通过 `streamer.put(token_ids)` 传递，流式对象负责任何进一步的处理。\n",
    "        model_kwargs:\n",
    "            额外的模型特定 kwargs 将传递给模型的 `forward` 函数。如果模型是编码器-解码器模型，kwargs 应包括 `encoder_outputs`。\n",
    "\n",
    "    Return:\n",
    "        [`~generation.GenerateDecoderOnlyOutput`], [`~generation.GenerateEncoderDecoderOutput`] or `torch.LongTensor`:\n",
    "        A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "        [`~generation.GenerateDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
    "        `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n",
    "        `model.config.is_encoder_decoder=True`.\n",
    "    \"\"\"\n",
    "    # 先拿出一些变量\n",
    "    pad_token_id = generation_config._pad_token_tensor  # pad 值\n",
    "    output_attentions = generation_config.output_attentions  # 是否输出 attentions\n",
    "    output_hidden_states = generation_config.output_hidden_states  # 是否输出 hidden states\n",
    "    output_scores = generation_config.output_scores  # 是否输出 scores\n",
    "    output_logits = generation_config.output_logits  # 是否输出 logits\n",
    "    return_dict_in_generate = generation_config.return_dict_in_generate  # 是否返回 dict\n",
    "    max_length = generation_config.max_length  # 最大长度\n",
    "    has_eos_stopping_criteria = any(hasattr(criteria, \"eos_token_id\") for criteria in stopping_criteria)  # 检测有没有 eos 停止条件\n",
    "    do_sample = generation_config.do_sample  # 是否使用采样\n",
    "\n",
    "    # 这里先将对应值初始化为元组类型\n",
    "    scores = () if (return_dict_in_generate and output_scores) else None\n",
    "    raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "    # 如果是 encoder-decoder 模型，从 model_kwargs 里取出 encoder 的 attentions 和 hidden states\n",
    "    if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    batch_size, cur_len = input_ids.shape\n",
    "    # 是否 batch 内所有序列都生成完成的判断标志位\n",
    "    this_peer_finished = False\n",
    "    # 创建一个跟踪每个序列是否完成生成的变量\n",
    "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "    # 初始化位置序号，如果用了 cache，就从 cache 里取，没有就根据 input_ids 长度创建\n",
    "    # prefilling 阶段确定输入的长度\n",
    "    model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
    "\n",
    "    while self._has_unfinished_sequences(\n",
    "        this_peer_finished, synced_gpus, device=input_ids.device, cur_len=cur_len, max_length=max_length\n",
    "    ):\n",
    "        # 开始准备模型的输入\n",
    "        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # 准备变量输出控制（注意：一些模型不接受所有输出控制）\n",
    "        # 模型默认只输出token的logits，如果需要输出attentions和hidden_states，则需要在model_inputs中传入特殊参数\n",
    "        model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
    "        model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
    "\n",
    "        # 前向传播以获取下一个token\n",
    "        outputs = self(**model_inputs, return_dict=True)\n",
    "\n",
    "        # synced_gpus: 不要浪费资源运行我们不需要的代码；kwargs必须在跳过之前更新\n",
    "        model_kwargs = self._update_model_kwargs_for_generation(\n",
    "            outputs,\n",
    "            model_kwargs,\n",
    "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "        )\n",
    "        if synced_gpus and this_peer_finished:\n",
    "            continue\n",
    "\n",
    "        # 克隆是必要的，以避免保持对 outputs.logits 的引用，这可能会非常大，特别是对于第一次迭代（克隆本身总是很小）\n",
    "        # 取出最后一个token的logits\n",
    "        next_token_logits = outputs.logits.clone()[:, -1, :].float()\n",
    "        next_token_logits = next_token_logits.to(input_ids.device)\n",
    "\n",
    "        # 解码策略处理\n",
    "        next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "\n",
    "        # 如果需要返回则存储分数、注意力、隐藏状态\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_token_scores,)\n",
    "            if output_logits:\n",
    "                raw_logits += (next_token_logits,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # 随机采样\n",
    "        if do_sample:\n",
    "            probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "            # TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        # 贪婪搜索\n",
    "        else:\n",
    "            next_tokens = torch.argmax(next_token_scores, dim=-1)\n",
    "\n",
    "        # 如果存在 eos 停止条件，则将下一个 token 设置为 pad 值\n",
    "        if has_eos_stopping_criteria:\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "\n",
    "        # 更新生成的 ids、模型输入和长度\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        # 如果流式对象不为空，则将下一个 token 传递给流式对象\n",
    "        if streamer is not None:\n",
    "            streamer.put(next_tokens.cpu())\n",
    "\n",
    "        # 更新未完成序列的标志位\n",
    "        unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
    "        this_peer_finished = unfinished_sequences.max() == 0\n",
    "        cur_len += 1\n",
    "\n",
    "        # 需要删除 outputs.logits，因为第一次迭代时它可能非常大\n",
    "        # 否则，会保留对 outputs 的引用，这会保持 logits 在下一个迭代中存活\n",
    "        del outputs\n",
    "\n",
    "    # 如果流式对象不为空，则结束流式对象\n",
    "    if streamer is not None:\n",
    "        streamer.end()\n",
    "\n",
    "    # 如果需要返回则返回\n",
    "    if return_dict_in_generate:\n",
    "        if self.config.is_encoder_decoder:\n",
    "            return GenerateEncoderDecoderOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "        else:\n",
    "            return GenerateDecoderOnlyOutput(\n",
    "                sequences=input_ids,\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "    # 如果不需要返回则返回生成的ids\n",
    "    else:\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAM_SEARCH 束搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息传递快<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 调用函数生成\n",
    "generated_ids = model.generate(\n",
    "    top_k=5,\n",
    "    top_p=1,\n",
    "    max_new_tokens=64,\n",
    "    num_return_sequences=1,\n",
    "    output_scores=True,\n",
    "    output_logits=True,\n",
    "    return_dict_in_generate=True,\n",
    "    num_beams=2,\n",
    "    do_sample=False,\n",
    "    **model_inputs,\n",
    ")\n",
    "print(tokenizer.decode(generated_ids['sequences'][0][model_inputs.input_ids.shape[1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息传递快<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score1: \n",
      "精彩 100848 -1.76\n",
      "信息 27369 -1.89\n",
      "内容 43815 -2.51\n",
      "分享 93149 -2.64\n",
      "更新 50007 -3.14\n",
      "score2: \n",
      "精彩 100848 -1.76\n",
      "信息 27369 -1.89\n",
      "内容 43815 -2.51\n",
      "分享 93149 -2.64\n",
      "更新 50007 -3.14\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score1: \n",
      "无限 105066 -0.65\n",
      "每一天 114169 -3.15\n",
      "分享 93149 -3.15\n",
      "随时 102422 -3.4\n",
      "纷 100100 -3.4\n",
      "score2: \n",
      "传递 104793 -1.68\n",
      "交流 101069 -2.43\n",
      "及时 100667 -2.93\n",
      "大 26288 -3.18\n",
      "更新 50007 -3.31\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score1: \n",
      "微博 101216 -2.56\n",
      "聊 100281 -2.7\n",
      "界 97120 -2.76\n",
      "时 13343 -3.01\n",
      "多 42140 -3.2\n",
      "score2: \n",
      "快 99234 -0.04\n",
      "迅速 104015 -4.67\n",
      "速 94299 -4.92\n",
      "便捷 105499 -5.17\n",
      "快速 101098 -5.54\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score1: \n",
      "<|im_end|> 151645 -0.06\n",
      "如 29524 -4.11\n",
      "。 1773 -4.36\n",
      "， 3837 -4.93\n",
      "！ 6313 -6.24\n",
      "score2: \n",
      "<|im_end|> 151645 -0.25\n",
      "。 1773 -2.35\n",
      "吧 100003 -4.48\n",
      "！ 6313 -4.85\n",
      "界 97120 -4.98\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score1: \n",
      "<|im_end|> 151645 -0.0\n",
      "这个 99487 -9.56\n",
      "这 43288 -10.63\n",
      "<|im_start|> 151644 -11.28\n",
      " � 32181 -11.63\n",
      "score2: \n",
      "风 99208 -0.17\n",
      "飞 99723 -2.8\n",
      "电 38212 -3.55\n",
      "雷 96465 -3.67\n",
      "闪电 112182 -3.92\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# beam search每步输出\n",
    "print(tokenizer.decode(generated_ids['sequences'][0][model_inputs.input_ids.shape[1]:]))\n",
    "print('-'*100)\n",
    "for idx in range(len(generated_ids['scores'])):\n",
    "    score1 = torch.topk(generated_ids['scores'][idx][0], k=5)\n",
    "    score2 = torch.topk(generated_ids['scores'][idx][1], k=5)\n",
    "    print(\"score1: \")\n",
    "    for i in range(5):\n",
    "        print(tokenizer.decode(score1.indices[i].item()), score1.indices[i].item(), round(score1.values[i].item(), 2))\n",
    "    print(\"score2: \")\n",
    "    for i in range(5):\n",
    "        print(tokenizer.decode(score2.indices[i].item()), score2.indices[i].item(), round(score2.values[i].item(), 2))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeamScorer 抽象基类\n",
    "class BeamScorer(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all beam scorers that are used for [`~PreTrainedModel.beam_search`] and\n",
    "    [`~PreTrainedModel.beam_sample`].\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    @add_start_docstrings(PROCESS_INPUTS_DOCSTRING)\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        raise NotImplementedError(\"This is an abstract method.\")\n",
    "\n",
    "    @abstractmethod\n",
    "    @add_start_docstrings(FINALIZE_INPUTS_DOCSTRING)\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        max_length: int,\n",
    "        **kwargs,\n",
    "    ) -> torch.LongTensor:\n",
    "        raise NotImplementedError(\"This is an abstract method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeamSearchScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchScorer(BeamScorer):\n",
    "    r\"\"\"\n",
    "    [`BeamScorer`] 实现标准的 beam search 解码\n",
    "\n",
    "    Adapted in part from [Facebook's XLM beam search code]\n",
    "    (https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).\n",
    "\n",
    "    多样束搜索算法和实现的参考 [Ashwin Kalyan's DBS implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)\n",
    "\n",
    "    Args:\n",
    "        batch_size (`int`):\n",
    "            用于标准束搜索解码的 `input_ids` 的批量大小。\n",
    "        num_beams (`int`):\n",
    "            束搜索的束数。\n",
    "        device (`torch.device`):\n",
    "            定义此实例的设备类型 (*e.g.*, `\"cpu\"` 或 `\"cuda\"`)。\n",
    "        length_penalty (`float`, *optional*, defaults to 1.0):\n",
    "            用于基于束搜索生成的序列长度的指数惩罚。它作为指数应用于序列长度,然后用于除以序列的分数。由于分数是序列的对数似然(即负值),\n",
    "            因此 `length_penalty` > 0.0 会促进生成更长的序列,而 `length_penalty` < 0.0 则会鼓励生成更短的序列。\n",
    "        do_early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n",
    "            控制基于束的方法(如束搜索)的停止条件。它接受以下值:\n",
    "            `True`, 当有 `num_beams` 个完整候选项时立即停止生成; \n",
    "            `False`, 应用启发式方法, 当找到更好候选项的可能性很小时停止生成; \n",
    "            `\"never\"`, 只有在确定不可能有更好的候选项时束搜索过程才会停止(标准束搜索算法)。\n",
    "        num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n",
    "            在调用 [`~transformers.BeamSearchScorer.finalize`] 时返回的束假设的数量。\n",
    "        num_beam_groups (`int`, *optional*, defaults to 1):\n",
    "            将 `num_beams` 分成多个组, 以确保不同组束之间的多样性。\n",
    "            每个组的大小为 `num_beams // num_beam_groups`。\n",
    "        max_length (`int`, *optional*):\n",
    "            要生成的序列的最大长度。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        num_beams: int,\n",
    "        device: torch.device,\n",
    "        length_penalty: Optional[float] = 1.0,\n",
    "        do_early_stopping: Optional[Union[bool, str]] = False,\n",
    "        num_beam_hyps_to_keep: Optional[int] = 1,\n",
    "        num_beam_groups: Optional[int] = 1,\n",
    "        max_length: Optional[int] = None,\n",
    "    ):\n",
    "        self.num_beams = num_beams\n",
    "        self.device = device\n",
    "        self.length_penalty = length_penalty\n",
    "        self.do_early_stopping = do_early_stopping\n",
    "        self.num_beam_hyps_to_keep = num_beam_hyps_to_keep\n",
    "        self.num_beam_groups = num_beam_groups\n",
    "        self.group_size = self.num_beams // self.num_beam_groups  # 分组束搜索每组束的个数\n",
    "\n",
    "        self._is_init = False  # 貌似也没用上\n",
    "        # self._beam_hyps[i*self.num_beam_groups+j] 是第 i 个 mini-batch 中第 j 组的束假设。\n",
    "        # 如果未使用 group_beam_search, 列表包含 `batch_size` 个束假设。\n",
    "        self._beam_hyps = [\n",
    "            BeamHypotheses(\n",
    "                num_beams=self.group_size,\n",
    "                length_penalty=self.length_penalty,\n",
    "                early_stopping=self.do_early_stopping,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            for _ in range(batch_size * self.num_beam_groups)\n",
    "        ]\n",
    "        # self._done[i*self.num_beam_groups+j] 表示第 i 个 mini-batch 中第 j 组的束假设是否完成。\n",
    "        self._done = torch.tensor(\n",
    "            [False for _ in range(batch_size * self.num_beam_groups)], dtype=torch.bool, device=self.device\n",
    "        )\n",
    "\n",
    "        if not isinstance(num_beams, int) or num_beams <= 1:\n",
    "            raise ValueError(\n",
    "                f\"`num_beams` has to be an integer strictly greater than 1, but is {num_beams}. For `num_beams` == 1,\"\n",
    "                \" one should make use of `greedy_search` instead.\"\n",
    "            )\n",
    "\n",
    "        if not isinstance(num_beam_groups, int) or (num_beam_groups > num_beams) or (num_beams % num_beam_groups != 0):\n",
    "            raise ValueError(\n",
    "                \"`num_beam_groups` has to be an integer smaller or equal than `num_beams` and `num_beams` has to be\"\n",
    "                f\" divisible by `num_beam_groups`, but is {num_beam_groups} with `num_beams` being {num_beams}.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def is_done(self) -> bool:\n",
    "        return self._done.all()\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        next_scores: torch.FloatTensor,\n",
    "        next_tokens: torch.LongTensor,\n",
    "        next_indices: torch.LongTensor,\n",
    "        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "        group_index: Optional[int] = 0,\n",
    "        decoder_prompt_len: Optional[int] = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # 将 next_scores 计算的长度(包括提示)加到当前长度\n",
    "        cur_len = input_ids.shape[-1] + 1\n",
    "        batch_size = len(self._beam_hyps) // self.num_beam_groups  # 计算 batch 的大小\n",
    "\n",
    "        # 分组束搜索的验证，输入的 batch 大小除以每组束的个数是否等于 batch 的大小\n",
    "        if not (batch_size == (input_ids.shape[0] // self.group_size)):\n",
    "            if self.num_beam_groups > 1:\n",
    "                raise ValueError(\n",
    "                    f\"A group beam size of {input_ids.shape[0]} is used as the input, but a group beam \"\n",
    "                    f\"size of {self.group_size} is expected by the beam scorer.\"\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"A beam size of {input_ids.shape[0]} is used as the input, but a beam size of \"\n",
    "                    f\"{self.group_size} is expected by the beam scorer.\"\n",
    "                )\n",
    "\n",
    "        # 初始化下一层 beam 的分数、token 和索引\n",
    "        device = input_ids.device\n",
    "        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)\n",
    "        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)\n",
    "        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)\n",
    "\n",
    "        # 如果 eos_token_id 不为空，则将其转换为 tensor\n",
    "        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n",
    "            if isinstance(eos_token_id, int):\n",
    "                eos_token_id = [eos_token_id]\n",
    "            eos_token_id = torch.tensor(eos_token_id)\n",
    "\n",
    "        # 遍历每个 batch\n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_group_idx = batch_idx * self.num_beam_groups + group_index  # 分组束搜索，计算 batch 的组索引\n",
    "            if self._done[batch_group_idx]:  # 如果该 组/batch 已经生成结束，则跳过\n",
    "                if self.num_beams < len(self._beam_hyps[batch_group_idx]):  # 如果束的个数小于 num_beams，则抛出错误，正常生成应该不会出现这种情况\n",
    "                    raise ValueError(f\"Batch can only be done if at least {self.num_beams} beams have been generated\")\n",
    "                if eos_token_id is None or pad_token_id is None:\n",
    "                    raise ValueError(\"Generated beams >= num_beams -> eos_token_id and pad_token have to be defined\")\n",
    "                # pad the batch\n",
    "                # 如果当前 组/batch 生成结束，则用 pad 值填充，将当前 batch 的 beam 的分数、索引都设置为 0\n",
    "                next_beam_scores[batch_idx, :] = 0\n",
    "                next_beam_tokens[batch_idx, :] = pad_token_id\n",
    "                next_beam_indices[batch_idx, :] = 0\n",
    "                continue\n",
    "\n",
    "            # 当前句子的下一个 token\n",
    "            beam_idx = 0\n",
    "            # 遍历当前 batch 的每个 beam 的 token\n",
    "            for beam_token_rank, (next_token, next_score, next_index) in enumerate(\n",
    "                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
    "            ):\n",
    "                batch_beam_idx = batch_idx * self.group_size + next_index  # 计算当前 batch 的 beam 的索引\n",
    "                # 如果 eos_token_id 不为空，并且当前 token 是 eos_token_id 之一，则将该 token 添加到结束生成的束中\n",
    "                if (eos_token_id is not None) and (next_token.item() in eos_token_id):\n",
    "                    # 分组束搜索，如果当前 token 不属于 top num_beams 个 token，则跳过\n",
    "                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size\n",
    "                    if is_beam_token_worse_than_top_num_beams:\n",
    "                        continue\n",
    "                    # 作为生成中间结果返回，如果 beam_indices 不为空，则将当前 batch 的 beam 的索引添加到 beam_indices 中\n",
    "                    if beam_indices is not None:\n",
    "                        beam_index = beam_indices[batch_beam_idx]\n",
    "                        beam_index = beam_index + (batch_beam_idx,)\n",
    "                    else:\n",
    "                        beam_index = None\n",
    "\n",
    "                    # 遇到eos，将其分数记录，用作后续判断是否停止\n",
    "                    self._beam_hyps[batch_group_idx].add(\n",
    "                        input_ids[batch_beam_idx].clone(),\n",
    "                        next_score.item(),\n",
    "                        beam_indices=beam_index,\n",
    "                        generated_len=cur_len - decoder_prompt_len,\n",
    "                    )\n",
    "                else:\n",
    "                    # 将下一个预测的 token 添加到束中, 因为它不是 eos_token\n",
    "                    next_beam_scores[batch_idx, beam_idx] = next_score\n",
    "                    next_beam_tokens[batch_idx, beam_idx] = next_token\n",
    "                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx\n",
    "                    beam_idx += 1\n",
    "\n",
    "                # 一旦束的下一个步骤已满, 不再添加更多 token\n",
    "                if beam_idx == self.group_size:\n",
    "                    break\n",
    "\n",
    "            # 分组束搜索，如果当前 batch 的 beam 的 token 个数小于每组束的个数，则抛出错误\n",
    "            if beam_idx < self.group_size:\n",
    "                raise ValueError(\n",
    "                    f\"At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id:\"\n",
    "                    f\" {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.\"\n",
    "                )\n",
    "\n",
    "            # 检查是否完成, 以便我们可以保存一个 pad 步骤(如果所有都完成)\n",
    "            self._done[batch_group_idx] = self._done[batch_group_idx] or self._beam_hyps[batch_group_idx].is_done(\n",
    "                next_scores[batch_idx].max().item(), cur_len, decoder_prompt_len\n",
    "            )\n",
    "\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"next_beam_scores\": next_beam_scores.view(-1),\n",
    "                \"next_beam_tokens\": next_beam_tokens.view(-1),\n",
    "                \"next_beam_indices\": next_beam_indices.view(-1),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def finalize(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        final_beam_scores: torch.FloatTensor,\n",
    "        final_beam_tokens: torch.LongTensor,\n",
    "        final_beam_indices: torch.LongTensor,\n",
    "        max_length: int,\n",
    "        pad_token_id: Optional[Union[int, torch.Tensor]] = None,\n",
    "        eos_token_id: Optional[Union[int, List[int], torch.Tensor]] = None,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "        decoder_prompt_len: Optional[int] = 0,\n",
    "    ) -> Tuple[torch.LongTensor]:\n",
    "        # 计算 batch 的大小\n",
    "        batch_size = len(self._beam_hyps) // self.num_beam_groups\n",
    "\n",
    "        # 如果 eos_token_id 不为空，则将其转换为 tensor\n",
    "        if eos_token_id is not None and not isinstance(eos_token_id, torch.Tensor):\n",
    "            if isinstance(eos_token_id, int):\n",
    "                eos_token_id = [eos_token_id]\n",
    "            eos_token_id = torch.tensor(eos_token_id)\n",
    "\n",
    "        # 完成所有打开的束假设并添加到生成的束假设中\n",
    "        for batch_group_idx, beam_hyp in enumerate(self._beam_hyps):\n",
    "            if self._done[batch_group_idx]:\n",
    "                continue\n",
    "\n",
    "            # 所有打开的束假设都添加到束假设中\n",
    "            # 束假设类自动保留最佳束\n",
    "            for index_per_group in range(self.group_size):\n",
    "                batch_beam_idx = batch_group_idx * self.group_size + index_per_group\n",
    "                final_score = final_beam_scores[batch_beam_idx].item()\n",
    "                final_tokens = input_ids[batch_beam_idx]\n",
    "                beam_index = beam_indices[batch_beam_idx] if beam_indices is not None else None\n",
    "                generated_len = final_tokens.shape[-1] - decoder_prompt_len\n",
    "                beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n",
    "\n",
    "        # 选择最佳束假设\n",
    "        sent_lengths = input_ids.new(batch_size * self.num_beam_hyps_to_keep)\n",
    "        best = []\n",
    "        best_indices = []\n",
    "        best_scores = torch.zeros(batch_size * self.num_beam_hyps_to_keep, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        # 检索最佳束假设\n",
    "        for i in range(batch_size):\n",
    "            beam_hyps_in_batch = self._beam_hyps[i * self.num_beam_groups : (i + 1) * self.num_beam_groups]\n",
    "            candidate_beams = [beam for beam_hyp in beam_hyps_in_batch for beam in beam_hyp.beams]\n",
    "            sorted_hyps = sorted(candidate_beams, key=lambda x: x[0])\n",
    "            for j in range(self.num_beam_hyps_to_keep):\n",
    "                best_hyp_tuple = sorted_hyps.pop()\n",
    "                best_score = best_hyp_tuple[0]\n",
    "                best_hyp = best_hyp_tuple[1]\n",
    "                best_index = best_hyp_tuple[2]\n",
    "                sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
    "\n",
    "                # 将束假设添加到列表中\n",
    "                best.append(best_hyp)\n",
    "\n",
    "                # 将索引添加到列表中\n",
    "                best_indices.append(best_index)\n",
    "\n",
    "                best_scores[i * self.num_beam_hyps_to_keep + j] = best_score\n",
    "\n",
    "        # 准备添加 eos\n",
    "        sent_lengths_max = sent_lengths.max().item() + 1\n",
    "        sent_max_len = min(sent_lengths_max, max_length) if max_length is not None else sent_lengths_max\n",
    "        decoded: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "\n",
    "        if len(best_indices) > 0 and best_indices[0] is not None:\n",
    "            indices: torch.LongTensor = input_ids.new(batch_size * self.num_beam_hyps_to_keep, sent_max_len)\n",
    "        else:\n",
    "            indices = None\n",
    "\n",
    "        # 较短的批次如果需要则填充\n",
    "        if sent_lengths.min().item() != sent_lengths.max().item():\n",
    "            if pad_token_id is None:\n",
    "                raise ValueError(\"`pad_token_id` has to be defined\")\n",
    "            decoded.fill_(pad_token_id)\n",
    "\n",
    "        if indices is not None:\n",
    "            indices.fill_(-1)\n",
    "\n",
    "        # 将束假设和 eos_token_id 填充到解码器中(如果后者适合)\n",
    "        for i, (hypo, best_idx) in enumerate(zip(best, best_indices)):\n",
    "            decoded[i, : sent_lengths[i]] = hypo\n",
    "\n",
    "            if indices is not None:\n",
    "                indices[i, : len(best_idx)] = torch.tensor(best_idx)\n",
    "\n",
    "            if sent_lengths[i] < sent_max_len:\n",
    "                # 仅插入第一个 eos_token_id\n",
    "                decoded[i, sent_lengths[i]] = eos_token_id[0]\n",
    "\n",
    "        return UserDict(\n",
    "            {\n",
    "                \"sequences\": decoded,\n",
    "                \"sequence_scores\": best_scores,\n",
    "                \"beam_indices\": indices,\n",
    "            }\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeamHypotheses 束假设类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamHypotheses:\n",
    "    def __init__(self, num_beams: int, length_penalty: float, early_stopping: bool, max_length: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        初始化束假设列表\n",
    "        \"\"\"\n",
    "        self.length_penalty = length_penalty  # 长度惩罚\n",
    "        self.early_stopping = early_stopping  # 是否提前停止\n",
    "        self.max_length = max_length  # 最大长度\n",
    "        self.num_beams = num_beams  # 束的个数\n",
    "        self.beams = []  # 束假设列表\n",
    "        self.worst_score = 1e9  # 最差分数\n",
    "\n",
    "        # 检查 early_stopping 是否为 bool 类型，并且 max_length 是否为 None\n",
    "        if not isinstance(self.early_stopping, bool) and self.max_length is None:\n",
    "            raise ValueError(\n",
    "                \"When `do_early_stopping` is set to a string, `max_length` must be defined. Ensure it is passed to the\"\n",
    "                \" BeamScorer class instance at initialization time.\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回束假设列表的长度\n",
    "        \"\"\"\n",
    "        return len(self.beams)\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        hyp: torch.LongTensor,\n",
    "        sum_logprobs: float,\n",
    "        beam_indices: Optional[torch.LongTensor] = None,\n",
    "        generated_len: Optional[int] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        添加一个新的束假设到列表中\n",
    "        \"\"\"\n",
    "        # 遇到eos的时候，计算当前束的停止分数\n",
    "        if generated_len is not None:\n",
    "            score = sum_logprobs / (generated_len**self.length_penalty)  # 计算束假设的分数\n",
    "        # 如果 generated_len 为空，则使用 hyp 的长度计算束假设的分数\n",
    "        else:\n",
    "            score = sum_logprobs / (hyp.shape[-1] ** self.length_penalty)  # 计算束假设的分数\n",
    "\n",
    "        # 如果束假设列表的长度小于束的个数，或者当前束的分数大于最差分数，则将当前束假设添加到列表中\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            self.beams.append((score, hyp, beam_indices))\n",
    "            # 如果束假设列表的长度大于束的个数，则删除最差的束假设\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_next_scores = sorted([(s, idx) for idx, (s, _, _) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_next_scores[0][1]]\n",
    "                self.worst_score = sorted_next_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs: float, cur_len: int, decoder_prompt_len: Optional[int] = 0) -> bool:\n",
    "        \"\"\"\n",
    "        如果束假设列表的长度大于束的个数，并且当前束的分数大于最差分数，则返回 True，表示生成结束\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "\n",
    "        # `True`: 遇到至少 `num_beams` 个束假设后直接结束\n",
    "        if self.early_stopping is True:\n",
    "            return True\n",
    "        # `False`: 启发式 -- 计算最高可能的分数，即使它不完全准确\n",
    "        # 当 `length_penalty` 为正时。更多信息，请参阅 https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565\n",
    "        # 如果没有额外设置参数，则进入该分支\n",
    "        elif self.early_stopping is False:\n",
    "            # 计算最高可达到的分数\n",
    "            highest_attainable_score = best_sum_logprobs / (cur_len - decoder_prompt_len) ** self.length_penalty\n",
    "            # 如果最差分数大于最高可达到的分数，则返回 True，表示生成结束\n",
    "            ret = self.worst_score >= highest_attainable_score\n",
    "            return ret\n",
    "        # `\"never\"`: 计算最高可能的分数，取决于 `length_penalty` 的信号\n",
    "        # 这里的区别在于，如果 `length_penalty` 为正，则使用 `max_length` 作为最大分母，否则使用 `cur_len` 作为最大分母\n",
    "        else:\n",
    "            # `length_penalty` > 0.0 -> max denominator is obtaned from `max_length`, not from `cur_len` -> min\n",
    "            # abs(`highest_attainable_score`) is obtained -> `highest_attainable_score` is negative, hence we obtain\n",
    "            # its max this way\n",
    "            if self.length_penalty > 0.0:\n",
    "                if self.max_length <= decoder_prompt_len:\n",
    "                    raise ValueError(\"max_length is not larger than decoder prompt length\")\n",
    "                highest_attainable_score = (\n",
    "                    best_sum_logprobs / (self.max_length - decoder_prompt_len) ** self.length_penalty\n",
    "                )\n",
    "            # 相反的逻辑在这里（从 `cur_len` 中获得最大 `highest_attainable_score`）\n",
    "            # 与 self.early_stopping is False 的使用相同\n",
    "            else:\n",
    "                highest_attainable_score = best_sum_logprobs / (cur_len - decoder_prompt_len) ** self.length_penalty\n",
    "            ret = self.worst_score >= highest_attainable_score\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _beam_search 束搜索函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beam_search(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor,\n",
    "    beam_scorer: BeamScorer,\n",
    "    logits_processor: LogitsProcessorList,\n",
    "    stopping_criteria: StoppingCriteriaList,\n",
    "    generation_config: GenerationConfig,\n",
    "    synced_gpus: bool,\n",
    "    **model_kwargs,\n",
    ") -> Union[GenerateBeamOutput, torch.LongTensor]:\n",
    "    r\"\"\"\n",
    "    使用 **beam search decoding** 生成序列的 token id，适用于具有语言建模头的文本解码器、文本到文本、语音到文本和视觉到文本模型。\n",
    "\n",
    "    Parameters:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            用于生成序列的输入序列。\n",
    "        beam_scorer (`BeamScorer`):\n",
    "            一个 [`BeamScorer`] 的派生实例，定义了如何构建、存储和排序束假设，在生成过程中。\n",
    "            更多信息，请参阅 [`BeamScorer`] 的文档。\n",
    "        logits_processor (`LogitsProcessorList`):\n",
    "            一个 [`LogitsProcessorList`] 的实例。列表包含从 [`LogitsProcessor`] 派生的类的实例，\n",
    "            用于在每个生成步骤中修改语言建模头的预测分数。\n",
    "        stopping_criteria (`StoppingCriteriaList`:\n",
    "            一个 [`StoppingCriteriaList`] 的实例。列表包含从 [`StoppingCriteria`] 派生的类的实例，\n",
    "            用于告诉生成循环是否应该停止。\n",
    "        generation_config ([`~generation.GenerationConfig`]):\n",
    "            用于解码方法的参数化。\n",
    "        synced_gpus (`bool`):\n",
    "            是否继续运行 while 循环直到 max_length（需要避免与 `FullyShardedDataParallel` 和 DeepSpeed ZeRO Stage 3 的死锁）。\n",
    "        model_kwargs:\n",
    "            额外的模型特定 kwargs 将转发到模型的 `forward` 函数。如果模型是 encoder-decoder 模型，kwargs 应包括 `encoder_outputs`。\n",
    "\n",
    "    Return:\n",
    "        [`generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] 或\n",
    "        `torch.LongTensor`: 一个 `torch.LongTensor` 包含生成的 token（默认行为）或\n",
    "        [`~generation.GenerateBeamDecoderOnlyOutput`] 如果 `model.config.is_encoder_decoder=False` 和\n",
    "        `return_dict_in_generate=True` 或一个 [`~generation.GenerateBeamEncoderDecoderOutput`] 如果\n",
    "        `model.config.is_encoder_decoder=True`.\n",
    "    \"\"\"\n",
    "    # 先拿出一些变量\n",
    "    pad_token_id = generation_config._pad_token_tensor  # pad 值\n",
    "    eos_token_id = generation_config._eos_token_tensor  # eos 值\n",
    "    output_attentions = generation_config.output_attentions  # 是否输出 attentions\n",
    "    output_hidden_states = generation_config.output_hidden_states  # 是否输出 hidden states\n",
    "    output_scores = generation_config.output_scores  # 是否输出 scores\n",
    "    output_logits = generation_config.output_logits  # 是否输出 logits\n",
    "    return_dict_in_generate = generation_config.return_dict_in_generate  # 是否返回 dict\n",
    "    sequential = generation_config.low_memory  # 是否顺序生成\n",
    "    do_sample = generation_config.do_sample  # 是否使用采样\n",
    "\n",
    "    batch_size = len(beam_scorer._beam_hyps)\n",
    "    num_beams = beam_scorer.num_beams\n",
    "\n",
    "    # batch_beam_size：batch_size * num_beams，扩展之后的 batch_size 大小\n",
    "    batch_beam_size, cur_len = input_ids.shape\n",
    "    # prefilling 阶段确定输入的长度\n",
    "    model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
    "\n",
    "    # 检查 batch_beam_size 大小是否正确\n",
    "    if num_beams * batch_size != batch_beam_size:\n",
    "        raise ValueError(\n",
    "            f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "        )\n",
    "\n",
    "    # 初始化 attention / hidden states / scores tuples\n",
    "    scores = () if (return_dict_in_generate and output_scores) else None\n",
    "    raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "    beam_indices = (\n",
    "        tuple(() for _ in range(batch_beam_size)) if (return_dict_in_generate and output_scores) else None\n",
    "    )\n",
    "    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "    # 如果模型是 encoder-decoder，获取 encoder attention weights 和 hidden states\n",
    "    if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    # 初始化第一个 beam 的分数为 0，其余 beam 的分数为 -1e9。这确保了只有第一个 beam 的 token 被考虑，以避免在所有 beam 中采样相同的 token。\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "    this_peer_finished = False\n",
    "\n",
    "    decoder_prompt_len = input_ids.shape[-1]  # 记录 prompt 长度\n",
    "\n",
    "    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
    "        # 准备模型输入，例如 position_ids、kv_cache、attention_mask\n",
    "        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # 准备可变输出控制（注意：某些模型可能不接受所有输出控制）\n",
    "        model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
    "        model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
    "\n",
    "        # 如果 sequential 为 True，将输入拆分为 batch_size 的批次并顺序运行\n",
    "        if sequential:\n",
    "            if any(\n",
    "                model_name in self.__class__.__name__.lower()\n",
    "                for model_name in [\n",
    "                    \"fsmt\",\n",
    "                    \"reformer\",\n",
    "                    \"ctrl\",\n",
    "                    \"gpt_bigcode\",\n",
    "                    \"transo_xl\",\n",
    "                    \"xlnet\",\n",
    "                    \"cpm\",\n",
    "                    \"jamba\",\n",
    "                ]\n",
    "            ):\n",
    "                raise RuntimeError(\n",
    "                    f\"Currently generation for {self.__class__.__name__} is not supported \"\n",
    "                    f\"for `low_memory beam_search`. Please open an issue on GitHub if you need this feature.\"\n",
    "                )\n",
    "\n",
    "            inputs_per_sub_batches = _split_model_inputs(\n",
    "                model_inputs,\n",
    "                split_size=batch_size,\n",
    "                full_batch_size=batch_beam_size,\n",
    "                config=self.config.get_text_config(),\n",
    "            )\n",
    "            outputs_per_sub_batch = [\n",
    "                self(**inputs_per_sub_batch, return_dict=True) for inputs_per_sub_batch in inputs_per_sub_batches\n",
    "            ]\n",
    "\n",
    "            outputs = stack_model_outputs(outputs_per_sub_batch, self.config.get_text_config())\n",
    "        # 正常的方式进行生成\n",
    "        else:  \n",
    "            outputs = self(**model_inputs, return_dict=True)\n",
    "\n",
    "        # 更新模型kwargs，将新生成的 attention_mask 和 kv_cache 添加到 model_kwargs 中\n",
    "        model_kwargs = self._update_model_kwargs_for_generation(\n",
    "            outputs,\n",
    "            model_kwargs,\n",
    "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "        )\n",
    "        # 同步GPU，不浪费资源运行我们不需要的代码\n",
    "        if synced_gpus and this_peer_finished:\n",
    "            cur_len = cur_len + 1\n",
    "            continue\n",
    "\n",
    "        # 取出最后一个 token 的 logits\n",
    "        # 克隆是必要的，以避免保持对 outputs.logits 的引用，这可能会非常大，尤其是在第一次迭代中（克隆本身总是很小）\n",
    "        # .float() 是必要的，以保留精度，以便稍后进行 logits 操作\n",
    "        next_token_logits = outputs.logits[:, -1, :].clone().float()\n",
    "        next_token_logits = next_token_logits.to(input_ids.device)\n",
    "        # 计算下一个 token 的分数，使用 log_softmax 函数，得到的值都是负数，将最后一个维度扩展成 vocab_size 的大小\n",
    "        next_token_scores = nn.functional.log_softmax(\n",
    "            next_token_logits, dim=-1\n",
    "        )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "        # 使用解码策略处理下一个 token 的分数\n",
    "        next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n",
    "        # 将处理后的分数与 beam_scores 相加，得到每个 token 的累加分数\n",
    "        next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
    "            next_token_scores_processed\n",
    "        )\n",
    "\n",
    "        # 如果需要返回分数、注意力、隐藏状态，则存储到对应的变量中\n",
    "        if return_dict_in_generate:\n",
    "            if output_scores:\n",
    "                scores += (next_token_scores_processed,)\n",
    "            if output_logits:\n",
    "                raw_logits += (next_token_logits,)\n",
    "            if output_attentions:\n",
    "                decoder_attentions += (\n",
    "                    (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                )\n",
    "                if self.config.is_encoder_decoder:\n",
    "                    cross_attentions += (outputs.cross_attentions,)\n",
    "            if output_hidden_states:\n",
    "                decoder_hidden_states += (\n",
    "                    (outputs.decoder_hidden_states,)\n",
    "                    if self.config.is_encoder_decoder\n",
    "                    else (outputs.hidden_states,)\n",
    "                )\n",
    "\n",
    "        # 将维度变成（batch_size, num_beams * vocab_size）\n",
    "        vocab_size = next_token_scores.shape[-1]\n",
    "        next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "\n",
    "        # beam search 策略：选择 1 + eos_token_id.shape[0] 个 token 作为每个 beam 的候选 token，\n",
    "        # 以确保每个 beam 至少有一个非 eos 的 token。\n",
    "        # 计算 eos_token_id 的数量，如果 eos_token_id 为 None，则设置为 0\n",
    "        n_eos_tokens = eos_token_id.shape[0] if eos_token_id is not None else 0\n",
    "        # 计算需要保留的 token 数量，至少保留 2 个 token，或者加上 eos_token_id 的数量\n",
    "        n_tokens_to_keep = max(2, 1 + n_eos_tokens) * num_beams\n",
    "        if do_sample:\n",
    "            # 使用 softmax 函数计算每个 token 的概率\n",
    "            probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "            # 使用多样式采样函数从概率分布中采样 token\n",
    "            next_tokens = torch.multinomial(probs, num_samples=n_tokens_to_keep)\n",
    "            # 使用 gather 函数从 next_token_scores 中收集采样到的 token 的分数\n",
    "            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n",
    "            # 对 next_token_scores 进行排序，以确保每个 beam 的分数从高到低排列\n",
    "            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n",
    "            # 使用 gather 函数从 next_tokens 中收集排序后的 token\n",
    "            next_tokens = torch.gather(next_tokens, -1, _indices)\n",
    "        else:\n",
    "            # 使用 topk 函数从 next_token_scores 中选择 n_tokens_to_keep 个最大的 token\n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "\n",
    "        # 计算 next_tokens 的索引是属于哪个 beam 的\n",
    "        next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "        # 计算 next_tokens 的实际值，即在词表中的索引\n",
    "        next_tokens = next_tokens % vocab_size\n",
    "\n",
    "        # 进行 beam search 的逻辑\n",
    "        beam_outputs = beam_scorer.process(\n",
    "            input_ids,\n",
    "            next_token_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            beam_indices=beam_indices,\n",
    "            decoder_prompt_len=decoder_prompt_len,\n",
    "        )\n",
    "        # 获取下一个 beam 的分数、token 和索引\n",
    "        beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "        beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "        beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "        # 将当前的 input_ids 和新的 token 连接起来\n",
    "        input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "        # 需要删除 outputs.logits，因为第一次迭代时它可能非常大\n",
    "        # 否则，会保留对 outputs 的引用，这会保持 logits 在下一个迭代中存活\n",
    "        # 重要：请注意，这应该出现在 _reorder_cache() 调用之前，以保存最大内存\n",
    "        # （这样内存峰值不会包括 outputs.logits）\n",
    "        del outputs\n",
    "\n",
    "        # 如果存在 past_key_values，则需要重新排序\n",
    "        if model_kwargs.get(\"past_key_values\", None) is not None:\n",
    "            model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n",
    "                model_kwargs[\"past_key_values\"], beam_idx\n",
    "            )\n",
    "\n",
    "        # 如果需要返回分数，则需要重新排序 beam_indices\n",
    "        if return_dict_in_generate and output_scores:\n",
    "            beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n",
    "\n",
    "        # 增加 cur_len\n",
    "        cur_len = cur_len + 1\n",
    "\n",
    "        # 如果 beam_scorer 已经完成或者所有停止条件都满足，则设置 this_peer_finished 为 True\n",
    "        if beam_scorer.is_done or all(stopping_criteria(input_ids, scores)):\n",
    "            this_peer_finished = True\n",
    "\n",
    "    # 完成 beam search 的逻辑，此时进行回溯找到最优的序列\n",
    "    sequence_outputs = beam_scorer.finalize(\n",
    "        input_ids,\n",
    "        beam_scores,\n",
    "        next_tokens,\n",
    "        next_indices,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        max_length=stopping_criteria.max_length,\n",
    "        beam_indices=beam_indices,\n",
    "        decoder_prompt_len=decoder_prompt_len,\n",
    "    )\n",
    "\n",
    "    # 如果需要返回字典，则返回字典\n",
    "    if return_dict_in_generate:\n",
    "        if not output_scores:\n",
    "            sequence_outputs[\"sequence_scores\"] = None\n",
    "\n",
    "        if self.config.is_encoder_decoder:\n",
    "            return GenerateBeamEncoderDecoderOutput(\n",
    "                sequences=sequence_outputs[\"sequences\"],\n",
    "                sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                encoder_attentions=encoder_attentions,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                decoder_attentions=decoder_attentions,\n",
    "                cross_attentions=cross_attentions,\n",
    "                decoder_hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "        else:\n",
    "            return GenerateBeamDecoderOnlyOutput(\n",
    "                sequences=sequence_outputs[\"sequences\"],\n",
    "                sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                scores=scores,\n",
    "                logits=raw_logits,\n",
    "                beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                attentions=decoder_attentions,\n",
    "                hidden_states=decoder_hidden_states,\n",
    "                past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "            )\n",
    "    # 如果不需要返回字典，则返回生成的序列\n",
    "    else:\n",
    "        return sequence_outputs[\"sequences\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUP_BEAM_SEARCH 分组束搜索(待做)\n",
    "\n",
    "要求 diversity_penalty 不为 0, do_sample 必须为 False，num_beams%num_beam_groups 必须为 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _group_beam_search(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor,\n",
    "    beam_scorer: BeamScorer,\n",
    "    logits_processor: LogitsProcessorList,\n",
    "    stopping_criteria: StoppingCriteriaList,\n",
    "    generation_config: GenerationConfig,\n",
    "    synced_gpus: bool,\n",
    "    **model_kwargs,\n",
    "):\n",
    "    r\"\"\"\n",
    "    Generates sequences of token ids for models with a language modeling head using **diverse beam search\n",
    "    decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n",
    "\n",
    "    Parameters:\n",
    "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "            The sequence used as a prompt for the generation.\n",
    "        beam_scorer (`BeamScorer`):\n",
    "            An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n",
    "            sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n",
    "        logits_processor (`LogitsProcessorList`):\n",
    "            An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
    "            used to modify the prediction scores of the language modeling head applied at each generation step.\n",
    "        stopping_criteria (`StoppingCriteriaList`):\n",
    "            An instance of [`StoppingCriteriaList`]. List of instances of class derived from [`StoppingCriteria`]\n",
    "            used to tell if the generation loop should stop.\n",
    "        generation_config ([`~generation.GenerationConfig`]):\n",
    "            The generation configuration to be used as parametrization of the decoding method.\n",
    "        synced_gpus (`bool`):\n",
    "            Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n",
    "            `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n",
    "        model_kwargs:\n",
    "            Additional model specific kwargs that will be forwarded to the `forward` function of the model. If\n",
    "            model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n",
    "\n",
    "    Return:\n",
    "        [`~generation.GenerateBeamDecoderOnlyOutput`], [`~generation.GenerateBeamEncoderDecoderOutput`] or\n",
    "        `torch.LongTensor`: A `torch.LongTensor` containing the generated tokens (default behaviour) or a\n",
    "        [`~generation.GenerateBeamDecoderOnlyOutput`] if `model.config.is_encoder_decoder=False` and\n",
    "        `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n",
    "        `model.config.is_encoder_decoder=True`.\n",
    "    \"\"\"\n",
    "    # init values\n",
    "    pad_token_id = generation_config._pad_token_tensor\n",
    "    eos_token_id = generation_config._eos_token_tensor\n",
    "    output_attentions = generation_config.output_attentions\n",
    "    output_hidden_states = generation_config.output_hidden_states\n",
    "    output_scores = generation_config.output_scores\n",
    "    output_logits = generation_config.output_logits\n",
    "    return_dict_in_generate = generation_config.return_dict_in_generate\n",
    "\n",
    "    num_beams = beam_scorer.num_beams\n",
    "    num_beam_groups = beam_scorer.num_beam_groups\n",
    "    num_sub_beams = num_beams // num_beam_groups\n",
    "    batch_size = len(beam_scorer._beam_hyps) // num_beam_groups\n",
    "    device = input_ids.device\n",
    "\n",
    "    batch_beam_size, cur_len = input_ids.shape\n",
    "    model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
    "\n",
    "    # 与束搜索类似操作，但代码未统一\n",
    "    if return_dict_in_generate and output_scores:\n",
    "        beam_indices = [tuple(() for _ in range(num_sub_beams * batch_size)) for _ in range(num_beam_groups)]\n",
    "    else:\n",
    "        beam_indices = None\n",
    "\n",
    "    if num_beams * batch_size != batch_beam_size:\n",
    "        raise ValueError(\n",
    "            f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n",
    "        )\n",
    "\n",
    "    # init attention / hidden states / scores tuples\n",
    "    scores = () if (return_dict_in_generate and output_scores) else None\n",
    "    raw_logits = () if (return_dict_in_generate and output_logits) else None\n",
    "    decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n",
    "    decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n",
    "\n",
    "    # 如果模型是编码器-解码器模型，则获取编码器的注意力权重和隐藏状态\n",
    "    if return_dict_in_generate and self.config.is_encoder_decoder:\n",
    "        encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n",
    "        encoder_hidden_states = (\n",
    "            model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n",
    "        )\n",
    "\n",
    "    # 初始化每个束组的第一个束的分数为0，其余的束的分数为-1e9。这确保了同一组中的束不会每次都生成相同的token。\n",
    "    beam_scores = torch.full((batch_size, num_beams), -1e9, dtype=torch.float, device=device)\n",
    "    # 将每个束组的第一个束的分数设置为0，这里::num_sub_beams代表每隔num_sub_beams步设置为0\n",
    "    beam_scores[:, ::num_sub_beams] = 0\n",
    "    # 将束的分数展平为一维\n",
    "    beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "    this_peer_finished = False\n",
    "\n",
    "    decoder_prompt_len = input_ids.shape[-1]  # record the prompt length of decoder\n",
    "    while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
    "        # predicted tokens in cur_len step\n",
    "        current_tokens = torch.zeros(batch_size * num_beams, dtype=input_ids.dtype, device=device)\n",
    "\n",
    "        # indices which will form the beams in the next time step\n",
    "        reordering_indices = torch.zeros(batch_size * num_beams, dtype=torch.long, device=device)\n",
    "\n",
    "        # do one decoder step on all beams of all sentences in batch\n",
    "        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "        # prepare variable output controls (note: some models won't accept all output controls)\n",
    "        model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n",
    "        model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
    "\n",
    "        outputs = self(**model_inputs, return_dict=True)\n",
    "\n",
    "        # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n",
    "        model_kwargs = self._update_model_kwargs_for_generation(\n",
    "            outputs,\n",
    "            model_kwargs,\n",
    "            is_encoder_decoder=self.config.is_encoder_decoder,\n",
    "        )\n",
    "        if synced_gpus and this_peer_finished:\n",
    "            cur_len = cur_len + 1\n",
    "            continue\n",
    "\n",
    "        if output_scores:\n",
    "            processed_score = torch.zeros_like(outputs.logits[:, -1, :])\n",
    "        if output_logits:\n",
    "            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n",
    "            # (the clone itself is always small)\n",
    "            raw_logit_score = outputs.logits[:, -1, :].clone()\n",
    "            raw_logit_score = raw_logit_score.to(input_ids.device)\n",
    "\n",
    "        for beam_group_idx in range(num_beam_groups):\n",
    "            # 计算当前束组的起始索引和结束索引\n",
    "            group_start_idx = beam_group_idx * num_sub_beams\n",
    "            # 计算当前束组的结束索引，这里min是防止group_start_idx + num_sub_beams超出num_beams，但属于重复判断，前面校验已经很严格了\n",
    "            group_end_idx = min(group_start_idx + num_sub_beams, num_beams)\n",
    "            # 计算当前束组的大小\n",
    "            group_size = group_end_idx - group_start_idx\n",
    "\n",
    "            # indices of beams of current group among all sentences in batch\n",
    "            batch_group_indices = []\n",
    "\n",
    "            for batch_idx in range(batch_size):\n",
    "                batch_group_indices.extend(\n",
    "                    [batch_idx * num_beams + idx for idx in range(group_start_idx, group_end_idx)]\n",
    "                )\n",
    "            group_input_ids = input_ids[batch_group_indices]\n",
    "\n",
    "            # select outputs of beams of current group only\n",
    "            # No need to clone() the logits here as they will not retain outputs.logits at the end of the loop\n",
    "            # .float() is needed to retain precision for later logits manipulations\n",
    "            next_token_logits = outputs.logits[batch_group_indices, -1, :].float()\n",
    "            next_token_logits = next_token_logits.to(input_ids.device)\n",
    "\n",
    "            next_token_scores = nn.functional.log_softmax(\n",
    "                next_token_logits, dim=-1\n",
    "            )  # (batch_size * group_size, vocab_size)\n",
    "            vocab_size = next_token_scores.shape[-1]\n",
    "\n",
    "            next_token_scores_processed = logits_processor(\n",
    "                group_input_ids, next_token_scores, current_tokens=current_tokens, beam_group_idx=beam_group_idx\n",
    "            )\n",
    "            next_token_scores = next_token_scores_processed + beam_scores[batch_group_indices].unsqueeze(-1)\n",
    "            next_token_scores = next_token_scores.expand_as(next_token_scores_processed)\n",
    "\n",
    "            if output_scores:\n",
    "                processed_score[batch_group_indices] = next_token_scores_processed\n",
    "\n",
    "            # reshape for beam search\n",
    "            next_token_scores = next_token_scores.view(batch_size, group_size * vocab_size)\n",
    "\n",
    "            # Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.\n",
    "            n_eos_tokens = eos_token_id.shape[0] if eos_token_id is not None else 0\n",
    "            next_token_scores, next_tokens = torch.topk(\n",
    "                next_token_scores, max(2, 1 + n_eos_tokens) * group_size, dim=1, largest=True, sorted=True\n",
    "            )\n",
    "\n",
    "            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "\n",
    "            # stateless\n",
    "            process_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "            beam_outputs = beam_scorer.process(\n",
    "                group_input_ids,\n",
    "                next_token_scores,\n",
    "                next_tokens,\n",
    "                next_indices,\n",
    "                pad_token_id=pad_token_id,\n",
    "                eos_token_id=eos_token_id,\n",
    "                beam_indices=process_beam_indices,\n",
    "                group_index=beam_group_idx,\n",
    "                decoder_prompt_len=decoder_prompt_len,\n",
    "            )\n",
    "                beam_scores[batch_group_indices] = beam_outputs[\"next_beam_scores\"]\n",
    "                beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "                beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "\n",
    "                if return_dict_in_generate and output_scores:\n",
    "                    beam_indices[beam_group_idx] = tuple(\n",
    "                        beam_indices[beam_group_idx][beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices[0]))\n",
    "                    )\n",
    "\n",
    "                input_ids[batch_group_indices] = group_input_ids[beam_idx]\n",
    "                group_input_ids = torch.cat([group_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "                current_tokens[batch_group_indices] = group_input_ids[:, -1]\n",
    "\n",
    "                # (beam_idx // group_size) -> batch_idx\n",
    "                # (beam_idx % group_size) -> offset of idx inside the group\n",
    "                reordering_indices[batch_group_indices] = (\n",
    "                    num_beams * torch.div(beam_idx, group_size, rounding_mode=\"floor\")\n",
    "                    + group_start_idx\n",
    "                    + (beam_idx % group_size)\n",
    "                )\n",
    "\n",
    "            # Store scores, attentions and hidden_states when required\n",
    "            if return_dict_in_generate:\n",
    "                if output_scores:\n",
    "                    scores += (processed_score,)\n",
    "                if output_logits:\n",
    "                    raw_logits += (raw_logit_score,)\n",
    "                if output_attentions:\n",
    "                    decoder_attentions += (\n",
    "                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n",
    "                    )\n",
    "                    if self.config.is_encoder_decoder:\n",
    "                        cross_attentions += (outputs.cross_attentions,)\n",
    "\n",
    "                if output_hidden_states:\n",
    "                    decoder_hidden_states += (\n",
    "                        (outputs.decoder_hidden_states,)\n",
    "                        if self.config.is_encoder_decoder\n",
    "                        else (outputs.hidden_states,)\n",
    "                    )\n",
    "\n",
    "            input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n",
    "\n",
    "            # This is needed to properly delete outputs.logits which may be very large for first iteration\n",
    "            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n",
    "            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n",
    "            # (that way the memory peak does not include outputs.logits)\n",
    "            del outputs\n",
    "\n",
    "            if model_kwargs.get(\"past_key_values\", None) is not None:\n",
    "                model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n",
    "                    model_kwargs[\"past_key_values\"], reordering_indices\n",
    "                )\n",
    "\n",
    "            # increase cur_len\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            if beam_scorer.is_done or all(stopping_criteria(input_ids, scores)):\n",
    "                this_peer_finished = True\n",
    "\n",
    "        final_beam_indices = sum(beam_indices, ()) if beam_indices is not None else None\n",
    "        sequence_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            max_length=stopping_criteria.max_length,\n",
    "            beam_indices=final_beam_indices,\n",
    "            decoder_prompt_len=decoder_prompt_len,\n",
    "        )\n",
    "\n",
    "        if return_dict_in_generate:\n",
    "            if not output_scores:\n",
    "                sequence_outputs[\"sequence_scores\"] = None\n",
    "\n",
    "            if self.config.is_encoder_decoder:\n",
    "                return GenerateBeamEncoderDecoderOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    encoder_attentions=encoder_attentions,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    decoder_attentions=decoder_attentions,\n",
    "                    cross_attentions=cross_attentions,\n",
    "                    decoder_hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "            else:\n",
    "                return GenerateBeamDecoderOnlyOutput(\n",
    "                    sequences=sequence_outputs[\"sequences\"],\n",
    "                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n",
    "                    scores=scores,\n",
    "                    logits=raw_logits,\n",
    "                    beam_indices=sequence_outputs[\"beam_indices\"],\n",
    "                    attentions=decoder_attentions,\n",
    "                    hidden_states=decoder_hidden_states,\n",
    "                    past_key_values=model_kwargs.get(\"past_key_values\"),\n",
    "                )\n",
    "        else:\n",
    "            return sequence_outputs[\"sequences\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
