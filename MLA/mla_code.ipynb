{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLA代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROPE+RMSNorm代码初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改编自：https://github.com/flashinfer-ai/flashinfer/blob/738460ff82e2230ebcc8dff50e49e1d6278e011a/tests/test_mla_decode_kernel.py\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    print('freqs_cis.shape', freqs_cis.shape)\n",
    "    print('x.shape', x.shape)\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "class DeepseekV2RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        DeepseekV2RMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return (self.weight * hidden_states).to(input_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLA 朴素版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 55.68700408935547\n"
     ]
    }
   ],
   "source": [
    "class DeepseekV2AttentionVanilla(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 以 deepseekv2 参数为准\n",
    "        self.hidden_size = 7168\n",
    "        self.num_heads = 128\n",
    "\n",
    "        self.q_lora_rank = 1536\n",
    "        self.qk_rope_head_dim = 64\n",
    "        self.kv_lora_rank = 512\n",
    "        self.v_head_dim = 128\n",
    "        self.qk_nope_head_dim = 128\n",
    "        self.q_head_dim = 192  # 192 = 128 + 64 = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "\n",
    "        self.rope_theta = 10000\n",
    "        self.q_a_layernorm = DeepseekV2RMSNorm(self.q_lora_rank)\n",
    "        self.softmax_scale = self.q_head_dim ** (-0.5)\n",
    "\n",
    "        # W^DQ ~ [7168, 1536]\n",
    "        self.W_DQ = nn.Linear(self.hidden_size, self.q_lora_rank, bias=False)\n",
    "        # W^UQ ~ [1536, 128*128]\n",
    "        self.W_UQ = nn.Linear(self.q_lora_rank, self.num_heads * self.qk_nope_head_dim, bias=False)\n",
    "        # W^QR ~ [1536, 128*64]\n",
    "        self.W_QR = nn.Linear(self.q_lora_rank, self.num_heads * self.qk_rope_head_dim, bias=False)\n",
    "        # W^KR ~ [1536, 64]\n",
    "        self.W_KR = nn.Linear(self.q_lora_rank, self.qk_rope_head_dim, bias=False)\n",
    "        # W^DKV ~ [7168, 512]\n",
    "        self.W_DKV = nn.Linear(self.hidden_size, self.kv_lora_rank, bias=False)\n",
    "        # W^UK ~ [512, 128*128]\n",
    "        self.W_UK = nn.Linear(self.kv_lora_rank, self.num_heads * self.qk_nope_head_dim, bias=False)\n",
    "        # W^UV ~ [512, 128*128]\n",
    "        self.W_UV = nn.Linear(self.kv_lora_rank, self.num_heads * self.v_head_dim, bias=False)\n",
    "        # W^O ~ [128*128, 7168]\n",
    "        self.W_O = nn.Linear(self.num_heads * self.v_head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "\n",
    "    def run_decode(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        compressed_kv_normed_cache: torch.Tensor,\n",
    "        k_pe_cache: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # 获取维度：[batch_size, query_length=1, hidden_size]\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        \n",
    "        # 当前step输入的x，经过W_DQ，得到 [batch_size, 1, q_lora_rank]\n",
    "        c_t_Q = self.q_a_layernorm(self.W_DQ(hidden_states))\n",
    "        # 再经过W_UQ，得到 [batch_size, 1, num_heads=128 * qk_nope_head_dim=128]\n",
    "        q_t_C = self.W_UQ(c_t_Q)\n",
    "        # 再经过W_QR，得到 [batch_size, 1, num_heads=128 * qk_rope_head_dim=64]\n",
    "        q_t_R = self.W_QR(c_t_Q).view(bsz, -1, self.num_heads, self.qk_rope_head_dim)\n",
    "        # 再经过W_KR，得到 [batch_size, 1, qk_rope_head_dim=64]\n",
    "        # 将当前step的k_t_R添加到k_pe_cache的最后一个位置，得到新的k_pe_cache\n",
    "        k_t_R = self.W_KR(c_t_Q)\n",
    "        k_pe_cache = torch.cat([k_pe_cache, k_t_R], dim=1)\n",
    "        # 将最后一个维度拆开，方便注意力计算\n",
    "        q_t_C = q_t_C.view(bsz, q_len, self.num_heads, self.qk_nope_head_dim).transpose(1, 2)\n",
    "\n",
    "        c_t_KV = self.W_DKV(hidden_states)\n",
    "        compressed_kv_normed_cache = torch.cat([compressed_kv_normed_cache, c_t_KV], dim=1)\n",
    "        k_C = self.W_UK(compressed_kv_normed_cache).view(bsz, -1, self.num_heads, self.qk_nope_head_dim).transpose(1, 2)\n",
    "        v_C = self.W_UV(compressed_kv_normed_cache).view(bsz, -1, self.num_heads, self.v_head_dim).transpose(1, 2)\n",
    "        \n",
    "        # 计算位置编码，暂时不用管，与其他的RoPE计算方式类似，最终得到旋转之后的 q_pe, k_pe\n",
    "        freqs_cis = precompute_freqs_cis(self.qk_rope_head_dim, compressed_kv_normed_cache.shape[1], self.rope_theta, use_scaled=False).to(q_t_R.device)\n",
    "        q_t_R, k_R = apply_rotary_emb(\n",
    "            q_t_R.repeat(1, compressed_kv_normed_cache.shape[1], 1, 1),\n",
    "            k_pe_cache.unsqueeze(2),\n",
    "            freqs_cis,\n",
    "        )\n",
    "        q_t_R = q_t_R[:, -1:, :, :].transpose(1, 2)\n",
    "        k_R = k_R.transpose(1, 2).repeat(1, self.num_heads, 1, 1)\n",
    "\n",
    "        attn_R = torch.matmul(q_t_R, k_R.transpose(2, 3))\n",
    "        attn_C = torch.matmul(q_t_C, k_C.transpose(2, 3))\n",
    "\n",
    "        attn_weights = (attn_R + attn_C) * self.softmax_scale\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q_t_C.dtype)\n",
    "\n",
    "        # 将注意力权重和v相乘，得到注意力输出，维度为[batch_size, num_heads, q_len, v_head_dim=128]\n",
    "        attn_output = torch.matmul(attn_weights, v_C)\n",
    "\n",
    "        # 将最后一个维度展开，得到[batch_size, num_heads, q_len, v_head_dim=128]\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(\n",
    "            bsz, q_len, self.num_heads * self.v_head_dim\n",
    "        )\n",
    "\n",
    "        # 将注意力输出和W^O相乘，得到最终的输出，维度为[batch_size, q_len, hidden_size=7168]\n",
    "        output = self.W_O(attn_output)\n",
    "\n",
    "        return output, attn_weights, compressed_kv_normed_cache, k_pe_cache\n",
    "\n",
    "mla_vanilla = DeepseekV2AttentionVanilla()\n",
    "\n",
    "batch_size = 6\n",
    "kv_len = 640\n",
    "\n",
    "hidden_states = torch.randn([batch_size, 1, mla_vanilla.hidden_size])\n",
    "compressed_kv_normed_cache = torch.randn([batch_size, kv_len, mla_vanilla.kv_lora_rank])\n",
    "k_pe_cache = torch.randn([batch_size, kv_len, mla_vanilla.qk_rope_head_dim])\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    output_vanilla, attn_weights, compressed_kv_normed_cache, k_pe_cache = mla_vanilla.run_decode(\n",
    "        hidden_states, compressed_kv_normed_cache, k_pe_cache\n",
    "    )\n",
    "    # print('output_vanilla.shape', output_vanilla.shape)\n",
    "    # print('attn_weights.shape', attn_weights.shape)\n",
    "    # print('compressed_kv_normed_cache.shape', compressed_kv_normed_cache.shape)\n",
    "    # print('k_pe_cache.shape', k_pe_cache.shape)\n",
    "    # print('-'*70)\n",
    "end_time = time.time()\n",
    "print('time', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\anaconda\\envs\\torch20\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2ForCausalLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLA 吸收矩阵版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 27.271135807037354\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class DeepseekV2AttentionMatAbsorbDecode(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 7168\n",
    "        self.num_heads = 128\n",
    "\n",
    "        self.q_lora_rank = 1536\n",
    "        self.qk_rope_head_dim = 64\n",
    "        self.kv_lora_rank = 512\n",
    "        self.v_head_dim = 128\n",
    "        self.qk_nope_head_dim = 128\n",
    "        self.q_head_dim = 192  # 192 = 128 + 64 = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "\n",
    "        self.rope_theta = 10000\n",
    "        self.q_a_layernorm = DeepseekV2RMSNorm(self.q_lora_rank)\n",
    "        self.softmax_scale = self.q_head_dim ** (-0.5)\n",
    "\n",
    "        # W^DQ ~ [7168, 1536]\n",
    "        self.W_DQ = nn.Linear(self.hidden_size, self.q_lora_rank, bias=False)\n",
    "        # W^UQ ~ [1536, 128*128]\n",
    "        self.W_UQ = nn.Linear(self.q_lora_rank, self.num_heads * self.qk_nope_head_dim, bias=False)\n",
    "        # W^QR ~ [1536, 128*64]\n",
    "        self.W_QR = nn.Linear(self.q_lora_rank, self.num_heads * self.qk_rope_head_dim, bias=False)\n",
    "        # W^KR ~ [1536, 64]\n",
    "        self.W_KR = nn.Linear(self.q_lora_rank, self.qk_rope_head_dim, bias=False)\n",
    "        # W^DKV ~ [7168, 512]\n",
    "        self.W_DKV = nn.Linear(self.hidden_size, self.kv_lora_rank, bias=False)\n",
    "        # W^UK ~ [512, 128*128]\n",
    "        self.W_UK = nn.Linear(self.kv_lora_rank, self.num_heads * self.qk_nope_head_dim, bias=False)\n",
    "        # W^UV ~ [512, 128*128]\n",
    "        self.W_UV = nn.Linear(self.kv_lora_rank, self.num_heads * self.v_head_dim, bias=False)\n",
    "        # W^O ~ [128*128, 7168]\n",
    "        self.W_O = nn.Linear(self.num_heads * self.v_head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "        # 由于nn.Linear初始化的时一个对象，权重矩阵只是对象中的一个类，没法直接两个矩阵相乘\n",
    "        # 所以需要用.weight来取出来，而且因为.weight的维度与初始化是反的，所以需要用t()来转置\n",
    "        # W_UQ_absorb ~ [1536, 128, 128]\n",
    "        W_UQ_absorb = self.W_UQ.weight.t().view(self.q_lora_rank, self.num_heads, self.qk_nope_head_dim)\n",
    "        # W_UK_absorb ~ [512, 128, 128]\n",
    "        W_UK_absorb = self.W_UK.weight.t().view(self.kv_lora_rank, self.num_heads, self.qk_nope_head_dim)\n",
    "        # W_UV_absorb ~ [512, 128, 128]\n",
    "        W_UV_absorb = self.W_UV.weight.t().view(self.kv_lora_rank, self.num_heads, self.v_head_dim)\n",
    "        # W_O_absorb ~ [7168, 128, 128]\n",
    "        W_O_absorb = self.W_O.weight.view(self.hidden_size, self.num_heads, self.v_head_dim)\n",
    "\n",
    "        # 吸收矩阵：将W_UQ和W_UK合并，得到新的W_UQK，维度为[1536, 128, 128]\n",
    "        # q~q_lora_rank  n~num_heads  d~qk_nope_head_dim  l~kv_lora_rank\n",
    "        # 这里把n当做batch_size，也就是矩阵相乘不会影响的那个维度，矩阵qd与dl相乘，得到ql，加上刚才的n，所以得到qnl\n",
    "        # 再将其flatten展平，得到[1536, 65536]\n",
    "        self.W_UQK = torch.einsum(\"q n d, l n d -> q n l\", W_UQ_absorb, W_UK_absorb).flatten(start_dim=1)\n",
    "        # 吸收矩阵，将W_UV和W_O合并，得到新的W_UV_O，维度为[128, 512, 7168]\n",
    "        # l~kv_lora_rank  n~num_heads  d~v_head_dim  h~hidden_size\n",
    "        # 这里把n当做batch_size，也就是矩阵相乘不会影响的那个维度，矩阵ld与dh相乘，得到lh，加上刚才的n，并把n放到最前面，所以得到nlh\n",
    "        # 再将其flatten展平，得到[65536, 7168]\n",
    "        self.W_UV_O = torch.einsum(\"l n d, h n d -> n l h\", W_UV_absorb, W_O_absorb).flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "    def run_decode(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        compressed_kv_normed_cache: torch.Tensor,\n",
    "        k_pe_cache: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # 获取维度：[batch_size, query_length=1, hidden_size]\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # 当前step输入的x，经过W_DQ，得到 [batch_size, 1, q_lora_rank]\n",
    "        c_t_Q = self.q_a_layernorm(self.W_DQ(hidden_states))\n",
    "        # 再经过W_UQ，得到 [batch_size, 1, num_heads=128 * qk_nope_head_dim=128]\n",
    "        q_t_C = torch.matmul(c_t_Q, self.W_UQK)\n",
    "        # 再经过W_QR，得到 [batch_size, 1, num_heads=128 * qk_rope_head_dim=64]\n",
    "        q_t_R = self.W_QR(c_t_Q).view(bsz, -1, self.num_heads, self.qk_rope_head_dim)\n",
    "        # 再经过W_KR，得到 [batch_size, 1, qk_rope_head_dim=64]\n",
    "        # 将当前step的k_t_R添加到k_pe_cache的最后一个位置，得到新的k_pe_cache\n",
    "        k_t_R = self.W_KR(c_t_Q)\n",
    "        k_pe_cache = torch.cat([k_pe_cache, k_t_R], dim=1)\n",
    "        # 将最后一个维度拆开，方便注意力计算\n",
    "        q_t_C = q_t_C.view(bsz, q_len, self.num_heads, self.kv_lora_rank).transpose(1, 2)\n",
    "\n",
    "        c_t_KV = self.W_DKV(hidden_states)\n",
    "        compressed_kv_normed_cache = torch.cat([compressed_kv_normed_cache, c_t_KV], dim=1)\n",
    "        \n",
    "        # 计算位置编码，暂时不用管，与其他的RoPE计算方式类似，最终得到旋转之后的 q_pe, k_pe\n",
    "        freqs_cis = precompute_freqs_cis(self.qk_rope_head_dim, compressed_kv_normed_cache.shape[1], self.rope_theta, use_scaled=False).to(q_t_R.device)\n",
    "        q_t_R, k_R = apply_rotary_emb(\n",
    "            q_t_R.repeat(1, compressed_kv_normed_cache.shape[1], 1, 1),\n",
    "            k_pe_cache.unsqueeze(2),\n",
    "            freqs_cis,\n",
    "        )\n",
    "        q_t_R = q_t_R[:, -1:, :, :].transpose(1, 2)\n",
    "        k_R = k_R.transpose(1, 2).repeat(1, self.num_heads, 1, 1)\n",
    "\n",
    "        attn_R = torch.matmul(q_t_R, k_R.transpose(2, 3))\n",
    "        attn_C = torch.matmul(q_t_C, compressed_kv_normed_cache.unsqueeze(1).transpose(2, 3))\n",
    "\n",
    "        attn_weights = (attn_R + attn_C) * self.softmax_scale\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q_t_C.dtype)\n",
    "\n",
    "        # attn_weights * c^KV * W^UVO\n",
    "        attn_output = torch.matmul(\n",
    "            attn_weights.squeeze(2),  # [bsz, 128, kv_len]\n",
    "            compressed_kv_normed_cache,  # [bsz, kv_len, 512]\n",
    "        ).reshape(bsz, self.num_heads * self.kv_lora_rank)\n",
    "        output = torch.matmul(attn_output, self.W_UV_O,)  # W_UV_O ~ [65536, 7168]\n",
    "\n",
    "        return output, attn_weights, compressed_kv_normed_cache, k_pe_cache\n",
    "        \n",
    "\n",
    "bsz = 6\n",
    "kv_len = 640\n",
    "page_size = 16\n",
    "\n",
    "hidden_states = torch.randn([bsz, 1, 7168])\n",
    "compressed_kv_normed_cache = torch.randn([bsz, kv_len, 512])\n",
    "k_pe_cache = torch.randn([bsz, kv_len, 64])\n",
    "\n",
    "mla_mat_absorb = DeepseekV2AttentionMatAbsorbDecode()\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(100):\n",
    "    output_vanilla, attn_weights, compressed_kv_normed_cache, k_pe_cache = mla_mat_absorb.run_decode(\n",
    "        hidden_states, compressed_kv_normed_cache, k_pe_cache\n",
    "    )\n",
    "    # print('output_vanilla.shape', output_vanilla.shape)\n",
    "    # print('attn_weights.shape', attn_weights.shape)\n",
    "    # print('compressed_kv_normed_cache.shape', compressed_kv_normed_cache.shape)\n",
    "    # print('k_pe_cache.shape', k_pe_cache.shape)\n",
    "    # print('-'*70)\n",
    "end_time = time.time()\n",
    "print('time', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算量对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_UQK吸收前： 168125005824\n",
      "W_UQK吸收后： 1411383296\n",
      "W_UV_O吸收前： 168217280512\n",
      "W_UV_O吸收后： 1780482048\n",
      "全部吸收前： 336342286336\n",
      "全部吸收后： 3191865344\n"
     ]
    }
   ],
   "source": [
    "# 实际吸收后的矩阵更大了，计算量更多了，但由于n的存在，序列越长，总体计算量越小\n",
    "n=20000\n",
    "W_UQK = 1536*128*128 + 512*128*128*n + 128*128*n\n",
    "W_UQK_absorbed = 1536*128*512 + 128*512*n\n",
    "W_UV_O = 512*128*128*n + 128*128*n + 128*128*7168\n",
    "W_UV_O_absorbed = 128*512*7168 + 128*512*n\n",
    "print('W_UQK吸收前：', W_UQK)\n",
    "print('W_UQK吸收后：', W_UQK_absorbed)\n",
    "print('W_UV_O吸收前：', W_UV_O)\n",
    "print('W_UV_O吸收后：', W_UV_O_absorbed)\n",
    "print('全部吸收前：', W_UQK + W_UV_O)\n",
    "print('全部吸收后：', W_UQK_absorbed + W_UV_O_absorbed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
